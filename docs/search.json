[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to CRISalid Deployment Documentation",
    "section": "",
    "text": "CRISalid is a modular set of open-source components designed to support the development of an institutional Current Research Information System (CRIS).\nIt is built to be message-oriented, modular, and loosely coupled. Each component communicates with others through asynchronous messaging (via RabbitMQ), while also exposing classical REST or GraphQL APIs to allow service-oriented approaches."
  },
  {
    "objectID": "index.html#development-test-deployment",
    "href": "index.html#development-test-deployment",
    "title": "Welcome to CRISalid Deployment Documentation",
    "section": "üß™ Development & Test Deployment",
    "text": "üß™ Development & Test Deployment\nUse this if you‚Äôre:\n\nWorking on a specific CRISalid module\nTesting integration locally\nEvaluating CRISalid for your institution\n\nWe provide:\n\nDocker Compose examples\nManual installation instructions (e.g., for deploying on virtual machines)\n\n‚Üí Explore Development & Test Deployment"
  },
  {
    "objectID": "index.html#production-deployment",
    "href": "index.html#production-deployment",
    "title": "Welcome to CRISalid Deployment Documentation",
    "section": "üöÄ Production Deployment",
    "text": "üöÄ Production Deployment\nFor institutional or production-grade deployments.\nWhile Kubernetes is the primary target, it is not required ‚Äî you may still use Docker or manual setups if needed.\nWe provide:\n\nShell scripts to generate Kubernetes manifests\nManual installation instructions for production environments\n\n‚Üí Explore Production Deployment"
  },
  {
    "objectID": "dev/dev-backend.html",
    "href": "dev/dev-backend.html",
    "title": "Using Docker Compose as Backend for Local Development",
    "section": "",
    "text": "This section explains how to use the Docker Compose stack strictly as a backend while running SoVisu+ locally  (Next.js dev server) for frontend development.\n\n\n\n\nRun all shared services (Neo4j, message bus, Keycloak, APIs, databases) in Docker, but run SoVisu+ on your host machine. To make this work, you must:\n\nMap the necessary service ports from containers to the host.\nUse a dedicated profile (e.g.¬†sovisuplus-db) to start only SoVisu+ backend services.\nPoint your local SoVisu+ to these services via env vars and /etc/hosts.\n\n\n\n\n\nAdd these entries to your /etc/hosts (if not already done in the main guide):\n127.0.0.1 sovisuplus.local\n127.0.0.1 keycloak.local\n\nSoVisu+ uses ORCID OAuth, which requires valid hostnames even for sandbox keys.\n\n\n\n\n\nYou‚Äôll run SoVisu+ locally, so your host needs to reach the containers. Make sure the following port mappings are enabled.\n\n\nUncomment the ports section so Postgres is reachable from your host:\n\n\nsovisuplus.yaml\n\nservices:\n  svp-db:\n    image: postgres:16\n    container_name: svp-db\n    restart: unless-stopped\n    environment:\n      POSTGRES_USER: ${SVP_DB_USER}\n      POSTGRES_PASSWORD: ${SVP_DB_PASSWORD}\n      POSTGRES_DB: ${SVP_DB_NAME}\n    expose:\n      - 5432\n    ports:\n      - 5432:5432\n    volumes:\n      - ./postgres-data:/var/lib/postgresql/data\n    networks:\n      - svp-network\n    healthcheck:\n      test: ['CMD-SHELL', 'pg_isready -d ${SVP_DB_NAME} -U ${SVP_DB_USER}']\n      interval: 1s\n      timeout: 5s\n      retries: 10\n    command: [\"postgres\",\"-c\",\"max_connections=400\",\"-c\",\"superuser_reserved_connections=3\"]\n    profiles:\n      - sovisuplus\n      - sovisuplus-db\n\n  sovisuplus:\n    image: crisalidesr/sovisuplus:latest\n    container_name: sovisuplus\n    ports:\n      - 3000:3000\n      - 3001:3001\n    environment:\n      - INIT_ROLES_ON_START=true\n      - RBAC_ROLES_FILE=/config/rbac.roles.yaml\n      - DB_NAME=${SVP_DB_NAME}\n      - DB_USER=${SVP_DB_USER}\n      - DB_PASSWORD=${SVP_DB_PASSWORD}\n      - DB_HOST=svp-db\n      - DB_PORT=5432\n      - WS_SCHEME=${SVP_WS_SCHEME}\n      - WS_HOST=${SVP_WS_HOST}\n      - WS_PORT=${SVP_WS_PORT}\n      - WS_PATH=${SVP_WS_PATH}\n      - WS_INTERNAL_PORT=${SVP_WS_INTERNAL_PORT}\n      - NEXTAUTH_SECRET=${NEXTAUTH_SECRET}\n      - AMQP_HOST=crisalid-bus\n      - AMQP_PORT=${CRISALID_BUS_AMQP_PORT}\n      - AMQP_USER=${CRISALID_BUS_USER}\n      - AMQP_PASSWORD=${CRISALID_BUS_PASSWORD}\n      - AMQP_QUEUE_NAME=${SVP_AMQP_QUEUE_NAME}\n      - AMQP_EXCHANGE_NAME=${SVP_AMQP_EXCHANGE_NAME}\n      - GRAPHQL_ENDPOINT_ENABLED=${GRAPHQL_ENDPOINT_ENABLED}\n      - GRAPHQL_ENDPOINT_URL=http://apollo:${APOLLO_API_PORT}/graphql\n      - GRAPHQL_API_KEY_ENABLED=${APOLLO_ENABLE_API_KEYS}\n      - GRAPHQL_API_KEY=${SOVISUPLUS_GRAPHQL_API_KEY}\n      - KEYCLOAK_CLIENT_ID=sovisuplus\n      - KEYCLOAK_CLIENT_SECRET=${SOVISUPLUS_KEYCLOAK_CLIENT_SECRET}\n      - KEYCLOAK_PUBLIC_ADDR=${KEYCLOAK_SCHEME}://${KEYCLOAK_HOST}:${KEYCLOAK_PORT}\n      - KEYCLOAK_INTERNAL_ADDR=http://keycloak:8080\n      - KEYCLOAK_REALM=${KEYCLOAK_REALM}\n      - APP_URL=${SOVISUPLUS_SCHEME}://${SOVISUPLUS_HOST}:${SOVISUPLUS_PORT}\n      - ORCID_URL=${ORCID_URL}\n      - ORCID_SCOPES=${ORCID_SCOPES}\n      - ORCID_CLIENT_ID=${ORCID_CLIENT_ID}\n      - ORCID_CLIENT_SECRET=${ORCID_CLIENT_SECRET}\n      - VOCABS_URL=${CRISALID_VOCAB_SEARCH_URL}\n      - NEXT_PUBLIC_AVAILABLE_VOCABS=${CRISALID_VOCAB_SEARCH_AVAILABLE_VOCABS:-jel,aat,acm,mesh}\n    depends_on:\n      svp-db:\n        condition: service_healthy\n      crisalid-bus:\n        condition: service_healthy\n    volumes:\n      - ./config:/config:ro\n      - ./theme:/custom-theme:ro\n    networks:\n      - svp-network\n      - crisalid-front\n    profiles:\n      - sovisuplus\n\nnetworks:\n  svp-network:\n    driver: bridge\n  crisalid-front:\n    driver: bridge\n\nservices:\n  svp-db:\n    image: postgres:16\n    container_name: svp-db\n    restart: unless-stopped\n    environment:\n      POSTGRES_USER: ${SVP_DB_USER}\n      POSTGRES_PASSWORD: ${SVP_DB_PASSWORD}\n      POSTGRES_DB: ${SVP_DB_NAME}\n    expose:\n      - 5432\n    ports:\n      - 5432:5432\n\n\n\nEnsure the public mapping is present (it typically is already because Apollo graphql GUI is one of the main user interfaces):\n\n\napollo.yaml\n\nservices:\n  apollo:\n    image: crisalidesr/crisalid-apollo:latest\n    ports:\n      - ${APOLLO_API_PORT}:4000\n    depends_on:\n      neo4j:\n        condition: service_healthy\n    environment:\n      - APP_ENV=DEV\n      - NEO4J_URI=bolt://neo4j:${NEO4J_BOLT_PORT}\n      - NEO4J_USER=${NEO4J_USER}\n      - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      - ENABLE_API_KEYS=${APOLLO_ENABLE_API_KEYS}\n      - API_KEYS=${SOVISUPLUS_GRAPHQL_API_KEY}\n    networks:\n      - ikg-network\n      - crisalid-front\n    profiles:\n      - apollo\nnetworks:\n  ikg-network:\n    driver: bridge\n  crisalid-front:\n    driver: bridge\n\nservices:\n  apollo:\n    image: crisalidesr/crisalid-apollo:latest\n    ports:\n      - ${APOLLO_API_PORT}:4000\n\nYour local SoVisu+ will call http://localhost:${APOLLO_API_PORT} (GraphQL endpoint /graphql).\n\n\n\n\nOnly the management UI port is exposed by default, but you can uncomment the AMQP port to allow external tools to connect:\n\n\ncrisalid-bus.yaml\n\nservices:\n  crisalid-bus:\n    image: rabbitmq:3-management\n    container_name: 'crisalid-bus'\n    environment:\n      - RABBITMQ_DEFAULT_USER=${CRISALID_BUS_USER}\n      - RABBITMQ_DEFAULT_PASS=${CRISALID_BUS_PASSWORD}\n      - RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS=-rabbit load_definitions \"${CRISALID_BUS_DEFINITIONS_FILE}\"\n    ports:\n      - \"${CRISALID_BUS_HTTP_PORT}:15672\"\n    #      - \"${CRISALID_BUS_AMQP_PORT}:5672\"\n    expose:\n      - \"${CRISALID_BUS_AMQP_PORT}\"\n    volumes:\n      - ./rabbitmq-data:/var/lib/rabbitmq\n      - ./rabbitmq-logs/:/var/log/rabbitmq\n      - ./definitions.json:${CRISALID_BUS_DEFINITIONS_FILE}:ro\n    healthcheck:\n      test: rabbitmq-diagnostics check_port_connectivity\n      interval: 1s\n      timeout: 3s\n      retries: 30\n    networks:\n      - crisalid-front\n      - crisalid-back\n    profiles:\n      - crisalid-bus\nnetworks:\n  crisalid-front:\n    driver: bridge\n  crisalid-back:\n    driver: bridge\n\nservices:\n  crisalid-bus:\n    image: rabbitmq:3-management\n    container_name: 'crisalid-bus'\n    environment:\n      - RABBITMQ_DEFAULT_USER=${CRISALID_BUS_USER}\n      - RABBITMQ_DEFAULT_PASS=${CRISALID_BUS_PASSWORD}\n      - RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS=-rabbit load_definitions \"${CRISALID_BUS_DEFINITIONS_FILE}\"\n    ports:\n      - \"${CRISALID_BUS_HTTP_PORT}:15672\"\n      - \"${CRISALID_BUS_AMQP_PORT}:5672\"\n    expose:\n      - \"${CRISALID_BUS_AMQP_PORT}\"\n\n\n\n\n\nReplace the sovisuplus profile with sovisuplus-db, which starts the DB and related services but not the SoVisu+ container itself.\nExample command:\ndocker compose \\\n  --profile cdb \\\n  --profile neo4j \\\n  --profile apollo \\\n  --profile crisalid-bus \\\n  --profile harvester \\\n  --profile ikg \\\n  --profile keycloak \\\n  --profile sovisuplus-db \\\n  up --remove-orphans\n\n\n\n\nStart your Next.js app on the host as usual (e.g., in the SoVisu+ repo):\nnpm run dev # for the main web gui\nnpm run dev:listener # for the backend listener\nMake sure your local env points to the Docker services. Typical variables (names vary by project):\n\nNEXT_PUBLIC_SUPPORTED_LOCALES=\"fr,en\"\n\nDATABASE_URL=\"postgresql://sovisuplus:sovisuplus_password@localhost:5432/sovisuplus?schema=public\"\n\nKEYCLOAK_CLIENT_ID=\"sovisuplus\"\nKEYCLOAK_CLIENT_SECRET=\"use-the-same-secret-as-in-docker-compose\"\nKEYCLOAK_ISSUER=\"http://keycloak.local:8080/realms/crisalid-inst\"\nNEXTAUTH_URL=\"http://sovisuplus.local:3000/api/auth\"\nNEXTAUTH_SECRET=\"use-a-secure-random-secret\"\n\nAMQP_USER=\"crisalid_bus_user\"\nAMQP_PASSWORD=\"use-the-same-password-as-in-docker-compose\"\nAMQP_HOST=\"localhost\"\nAMQP_PORT=\"5672\"\nAMQP_QUEUE_NAME=\"sovisuplus\"\nAMQP_EXCHANGE_NAME=\"graph\"\n\nGRAPHQL_ENDPOINT_ENABLED=\"true\"\nGRAPHQL_ENDPOINT_URL=\"http://localhost:4000/graphql\"\nGRAPHQL_API_KEY_ENABLED=\"false\"\nGRAPHQL_API_KEY=\"not-needed-in-dev\"\n\nPERSPECTIVES_ROLES_FILTER=\"author\"\nPUBLICATION_LIST_ROLES_FILTER=\"author\"\n\nORCID_URL=\"https://sandbox.orcid.org\"\nAPP_URL=\"http://sovisuplus.local:3000\"\nORCID_SCOPES=\"/person/update\"\nORCID_CLIENT_ID=\"use-the-same-client-id--provided-by-orcid-as-in-docker-compose\"\nORCID_CLIENT_SECRET=\"use-the-same-client-secret-provided-by-orcid-as-in-docker-compose\"\nVisit SoVisu+ at:\nhttp://sovisuplus.local:3000"
  },
  {
    "objectID": "dev/dev-backend.html#goal",
    "href": "dev/dev-backend.html#goal",
    "title": "Using Docker Compose as Backend for Local Development",
    "section": "",
    "text": "Run all shared services (Neo4j, message bus, Keycloak, APIs, databases) in Docker, but run SoVisu+ on your host machine. To make this work, you must:\n\nMap the necessary service ports from containers to the host.\nUse a dedicated profile (e.g.¬†sovisuplus-db) to start only SoVisu+ backend services.\nPoint your local SoVisu+ to these services via env vars and /etc/hosts."
  },
  {
    "objectID": "dev/dev-backend.html#required-hostnames",
    "href": "dev/dev-backend.html#required-hostnames",
    "title": "Using Docker Compose as Backend for Local Development",
    "section": "",
    "text": "Add these entries to your /etc/hosts (if not already done in the main guide):\n127.0.0.1 sovisuplus.local\n127.0.0.1 keycloak.local\n\nSoVisu+ uses ORCID OAuth, which requires valid hostnames even for sandbox keys."
  },
  {
    "objectID": "dev/dev-backend.html#open-the-right-ports-in-compose",
    "href": "dev/dev-backend.html#open-the-right-ports-in-compose",
    "title": "Using Docker Compose as Backend for Local Development",
    "section": "",
    "text": "You‚Äôll run SoVisu+ locally, so your host needs to reach the containers. Make sure the following port mappings are enabled.\n\n\nUncomment the ports section so Postgres is reachable from your host:\n\n\nsovisuplus.yaml\n\nservices:\n  svp-db:\n    image: postgres:16\n    container_name: svp-db\n    restart: unless-stopped\n    environment:\n      POSTGRES_USER: ${SVP_DB_USER}\n      POSTGRES_PASSWORD: ${SVP_DB_PASSWORD}\n      POSTGRES_DB: ${SVP_DB_NAME}\n    expose:\n      - 5432\n    ports:\n      - 5432:5432\n    volumes:\n      - ./postgres-data:/var/lib/postgresql/data\n    networks:\n      - svp-network\n    healthcheck:\n      test: ['CMD-SHELL', 'pg_isready -d ${SVP_DB_NAME} -U ${SVP_DB_USER}']\n      interval: 1s\n      timeout: 5s\n      retries: 10\n    command: [\"postgres\",\"-c\",\"max_connections=400\",\"-c\",\"superuser_reserved_connections=3\"]\n    profiles:\n      - sovisuplus\n      - sovisuplus-db\n\n  sovisuplus:\n    image: crisalidesr/sovisuplus:latest\n    container_name: sovisuplus\n    ports:\n      - 3000:3000\n      - 3001:3001\n    environment:\n      - INIT_ROLES_ON_START=true\n      - RBAC_ROLES_FILE=/config/rbac.roles.yaml\n      - DB_NAME=${SVP_DB_NAME}\n      - DB_USER=${SVP_DB_USER}\n      - DB_PASSWORD=${SVP_DB_PASSWORD}\n      - DB_HOST=svp-db\n      - DB_PORT=5432\n      - WS_SCHEME=${SVP_WS_SCHEME}\n      - WS_HOST=${SVP_WS_HOST}\n      - WS_PORT=${SVP_WS_PORT}\n      - WS_PATH=${SVP_WS_PATH}\n      - WS_INTERNAL_PORT=${SVP_WS_INTERNAL_PORT}\n      - NEXTAUTH_SECRET=${NEXTAUTH_SECRET}\n      - AMQP_HOST=crisalid-bus\n      - AMQP_PORT=${CRISALID_BUS_AMQP_PORT}\n      - AMQP_USER=${CRISALID_BUS_USER}\n      - AMQP_PASSWORD=${CRISALID_BUS_PASSWORD}\n      - AMQP_QUEUE_NAME=${SVP_AMQP_QUEUE_NAME}\n      - AMQP_EXCHANGE_NAME=${SVP_AMQP_EXCHANGE_NAME}\n      - GRAPHQL_ENDPOINT_ENABLED=${GRAPHQL_ENDPOINT_ENABLED}\n      - GRAPHQL_ENDPOINT_URL=http://apollo:${APOLLO_API_PORT}/graphql\n      - GRAPHQL_API_KEY_ENABLED=${APOLLO_ENABLE_API_KEYS}\n      - GRAPHQL_API_KEY=${SOVISUPLUS_GRAPHQL_API_KEY}\n      - KEYCLOAK_CLIENT_ID=sovisuplus\n      - KEYCLOAK_CLIENT_SECRET=${SOVISUPLUS_KEYCLOAK_CLIENT_SECRET}\n      - KEYCLOAK_PUBLIC_ADDR=${KEYCLOAK_SCHEME}://${KEYCLOAK_HOST}:${KEYCLOAK_PORT}\n      - KEYCLOAK_INTERNAL_ADDR=http://keycloak:8080\n      - KEYCLOAK_REALM=${KEYCLOAK_REALM}\n      - APP_URL=${SOVISUPLUS_SCHEME}://${SOVISUPLUS_HOST}:${SOVISUPLUS_PORT}\n      - ORCID_URL=${ORCID_URL}\n      - ORCID_SCOPES=${ORCID_SCOPES}\n      - ORCID_CLIENT_ID=${ORCID_CLIENT_ID}\n      - ORCID_CLIENT_SECRET=${ORCID_CLIENT_SECRET}\n      - VOCABS_URL=${CRISALID_VOCAB_SEARCH_URL}\n      - NEXT_PUBLIC_AVAILABLE_VOCABS=${CRISALID_VOCAB_SEARCH_AVAILABLE_VOCABS:-jel,aat,acm,mesh}\n    depends_on:\n      svp-db:\n        condition: service_healthy\n      crisalid-bus:\n        condition: service_healthy\n    volumes:\n      - ./config:/config:ro\n      - ./theme:/custom-theme:ro\n    networks:\n      - svp-network\n      - crisalid-front\n    profiles:\n      - sovisuplus\n\nnetworks:\n  svp-network:\n    driver: bridge\n  crisalid-front:\n    driver: bridge\n\nservices:\n  svp-db:\n    image: postgres:16\n    container_name: svp-db\n    restart: unless-stopped\n    environment:\n      POSTGRES_USER: ${SVP_DB_USER}\n      POSTGRES_PASSWORD: ${SVP_DB_PASSWORD}\n      POSTGRES_DB: ${SVP_DB_NAME}\n    expose:\n      - 5432\n    ports:\n      - 5432:5432\n\n\n\nEnsure the public mapping is present (it typically is already because Apollo graphql GUI is one of the main user interfaces):\n\n\napollo.yaml\n\nservices:\n  apollo:\n    image: crisalidesr/crisalid-apollo:latest\n    ports:\n      - ${APOLLO_API_PORT}:4000\n    depends_on:\n      neo4j:\n        condition: service_healthy\n    environment:\n      - APP_ENV=DEV\n      - NEO4J_URI=bolt://neo4j:${NEO4J_BOLT_PORT}\n      - NEO4J_USER=${NEO4J_USER}\n      - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      - ENABLE_API_KEYS=${APOLLO_ENABLE_API_KEYS}\n      - API_KEYS=${SOVISUPLUS_GRAPHQL_API_KEY}\n    networks:\n      - ikg-network\n      - crisalid-front\n    profiles:\n      - apollo\nnetworks:\n  ikg-network:\n    driver: bridge\n  crisalid-front:\n    driver: bridge\n\nservices:\n  apollo:\n    image: crisalidesr/crisalid-apollo:latest\n    ports:\n      - ${APOLLO_API_PORT}:4000\n\nYour local SoVisu+ will call http://localhost:${APOLLO_API_PORT} (GraphQL endpoint /graphql).\n\n\n\n\nOnly the management UI port is exposed by default, but you can uncomment the AMQP port to allow external tools to connect:\n\n\ncrisalid-bus.yaml\n\nservices:\n  crisalid-bus:\n    image: rabbitmq:3-management\n    container_name: 'crisalid-bus'\n    environment:\n      - RABBITMQ_DEFAULT_USER=${CRISALID_BUS_USER}\n      - RABBITMQ_DEFAULT_PASS=${CRISALID_BUS_PASSWORD}\n      - RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS=-rabbit load_definitions \"${CRISALID_BUS_DEFINITIONS_FILE}\"\n    ports:\n      - \"${CRISALID_BUS_HTTP_PORT}:15672\"\n    #      - \"${CRISALID_BUS_AMQP_PORT}:5672\"\n    expose:\n      - \"${CRISALID_BUS_AMQP_PORT}\"\n    volumes:\n      - ./rabbitmq-data:/var/lib/rabbitmq\n      - ./rabbitmq-logs/:/var/log/rabbitmq\n      - ./definitions.json:${CRISALID_BUS_DEFINITIONS_FILE}:ro\n    healthcheck:\n      test: rabbitmq-diagnostics check_port_connectivity\n      interval: 1s\n      timeout: 3s\n      retries: 30\n    networks:\n      - crisalid-front\n      - crisalid-back\n    profiles:\n      - crisalid-bus\nnetworks:\n  crisalid-front:\n    driver: bridge\n  crisalid-back:\n    driver: bridge\n\nservices:\n  crisalid-bus:\n    image: rabbitmq:3-management\n    container_name: 'crisalid-bus'\n    environment:\n      - RABBITMQ_DEFAULT_USER=${CRISALID_BUS_USER}\n      - RABBITMQ_DEFAULT_PASS=${CRISALID_BUS_PASSWORD}\n      - RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS=-rabbit load_definitions \"${CRISALID_BUS_DEFINITIONS_FILE}\"\n    ports:\n      - \"${CRISALID_BUS_HTTP_PORT}:15672\"\n      - \"${CRISALID_BUS_AMQP_PORT}:5672\"\n    expose:\n      - \"${CRISALID_BUS_AMQP_PORT}\""
  },
  {
    "objectID": "dev/dev-backend.html#use-the-backend-only-profile",
    "href": "dev/dev-backend.html#use-the-backend-only-profile",
    "title": "Using Docker Compose as Backend for Local Development",
    "section": "",
    "text": "Replace the sovisuplus profile with sovisuplus-db, which starts the DB and related services but not the SoVisu+ container itself.\nExample command:\ndocker compose \\\n  --profile cdb \\\n  --profile neo4j \\\n  --profile apollo \\\n  --profile crisalid-bus \\\n  --profile harvester \\\n  --profile ikg \\\n  --profile keycloak \\\n  --profile sovisuplus-db \\\n  up --remove-orphans"
  },
  {
    "objectID": "dev/dev-backend.html#run-sovisu-locally",
    "href": "dev/dev-backend.html#run-sovisu-locally",
    "title": "Using Docker Compose as Backend for Local Development",
    "section": "",
    "text": "Start your Next.js app on the host as usual (e.g., in the SoVisu+ repo):\nnpm run dev # for the main web gui\nnpm run dev:listener # for the backend listener\nMake sure your local env points to the Docker services. Typical variables (names vary by project):\n\nNEXT_PUBLIC_SUPPORTED_LOCALES=\"fr,en\"\n\nDATABASE_URL=\"postgresql://sovisuplus:sovisuplus_password@localhost:5432/sovisuplus?schema=public\"\n\nKEYCLOAK_CLIENT_ID=\"sovisuplus\"\nKEYCLOAK_CLIENT_SECRET=\"use-the-same-secret-as-in-docker-compose\"\nKEYCLOAK_ISSUER=\"http://keycloak.local:8080/realms/crisalid-inst\"\nNEXTAUTH_URL=\"http://sovisuplus.local:3000/api/auth\"\nNEXTAUTH_SECRET=\"use-a-secure-random-secret\"\n\nAMQP_USER=\"crisalid_bus_user\"\nAMQP_PASSWORD=\"use-the-same-password-as-in-docker-compose\"\nAMQP_HOST=\"localhost\"\nAMQP_PORT=\"5672\"\nAMQP_QUEUE_NAME=\"sovisuplus\"\nAMQP_EXCHANGE_NAME=\"graph\"\n\nGRAPHQL_ENDPOINT_ENABLED=\"true\"\nGRAPHQL_ENDPOINT_URL=\"http://localhost:4000/graphql\"\nGRAPHQL_API_KEY_ENABLED=\"false\"\nGRAPHQL_API_KEY=\"not-needed-in-dev\"\n\nPERSPECTIVES_ROLES_FILTER=\"author\"\nPUBLICATION_LIST_ROLES_FILTER=\"author\"\n\nORCID_URL=\"https://sandbox.orcid.org\"\nAPP_URL=\"http://sovisuplus.local:3000\"\nORCID_SCOPES=\"/person/update\"\nORCID_CLIENT_ID=\"use-the-same-client-id--provided-by-orcid-as-in-docker-compose\"\nORCID_CLIENT_SECRET=\"use-the-same-client-secret-provided-by-orcid-as-in-docker-compose\"\nVisit SoVisu+ at:\nhttp://sovisuplus.local:3000"
  },
  {
    "objectID": "dev/backup-restore-with-dc.html",
    "href": "dev/backup-restore-with-dc.html",
    "title": "Data backup and restoration with Docker Compose",
    "section": "",
    "text": "Quick recipes to backup and restore services in the CRISalid Docker Compose stack."
  },
  {
    "objectID": "dev/backup-restore-with-dc.html#full-backup",
    "href": "dev/backup-restore-with-dc.html#full-backup",
    "title": "Data backup and restoration with Docker Compose",
    "section": "üì¶ Full backup",
    "text": "üì¶ Full backup\n\nRecommended: stop Neo4j before a backup to ensure a clean snapshot.\n\nCreate a compressed archive of the key data directories (preserve numeric ownership for container UID/GID 7474):\nsudo tar --numeric-owner -czvf neo4j-full-$(date +%F).tar.gz \\\n  data import backups plugins"
  },
  {
    "objectID": "dev/backup-restore-with-dc.html#restore-from-a-backup-archive",
    "href": "dev/backup-restore-with-dc.html#restore-from-a-backup-archive",
    "title": "Data backup and restoration with Docker Compose",
    "section": "‚ôªÔ∏è Restore from a backup archive",
    "text": "‚ôªÔ∏è Restore from a backup archive\n\nImportant: Stop Neo4j before restoring files.\n\n\nExtract the archive to the Neo4j docker directory:\n\ntar -xzvf neo4j-data-20250908.tar.gz \\\n  -C ~/.../crisalid-deployment/docker/neo4j\nAlternatively, if you have full access to the server, you can skip the previous steps and restore directly with rsync.\ncd /path/to/your/local/crisalid-deployment/docker/neo4j\n\nsudo rsync -av --delete \\\n  myuser@myserver.myuniv.fr:/opt/crisalid/crisalid-deployment/docker/neo4j/data/ \\\n  ./data/\n\nEnsure directory ownership matches the container user (UID/GID 7474):\n\ncd /path/to/your/local/crisalid-deployment/docker/neo4j\nsudo chown -R 7474:7474 data import backups plugins\n\nStart Neo4j again (alone or with other services):\n\ndocker compose --profile neo4j up -d neo4j\n\n(Optional) Check logs:\n\ndocker compose logs -f neo4j"
  },
  {
    "objectID": "dev/backup-restore-with-dc.html#make-a-sql-dump",
    "href": "dev/backup-restore-with-dc.html#make-a-sql-dump",
    "title": "Data backup and restoration with Docker Compose",
    "section": "Make a SQL dump",
    "text": "Make a SQL dump\nWith a running Docker Compose stack (svp-db container):\n# optional: export env if not already loaded\nexport SVP_DB_NAME=${SVP_DB_NAME}\nexport SVP_DB_USER=${SVP_DB_USER}\n\ndocker compose exec -T svp-db \\\n  pg_dump -U \"$SVP_DB_USER\" -d \"$SVP_DB_NAME\" \\\n    --format=plain \\\n    --no-owner --no-privileges \\\n    --clean --if-exists \\\n  &gt; \"sovisuplus-$(date +%F).sql\""
  },
  {
    "objectID": "dev/backup-restore-with-dc.html#restore-into-docker-same-svp-db",
    "href": "dev/backup-restore-with-dc.html#restore-into-docker-same-svp-db",
    "title": "Data backup and restoration with Docker Compose",
    "section": "Restore into Docker (same svp-db)",
    "text": "Restore into Docker (same svp-db)\ncat sovisuplus-YYYY-MM-DD.sql | \\\n  docker compose exec -T svp-db psql -U \"$SVP_DB_USER\" -d \"$SVP_DB_NAME\""
  },
  {
    "objectID": "dev/backup-restore-with-dc.html#restore-into-local-postgres-on-your-machine",
    "href": "dev/backup-restore-with-dc.html#restore-into-local-postgres-on-your-machine",
    "title": "Data backup and restoration with Docker Compose",
    "section": "Restore into local Postgres (on your machine)",
    "text": "Restore into local Postgres (on your machine)\nPGPASSWORD=\"$SVP_DB_PASSWORD\" psql \\\n  -h localhost -p 5432 -U \"$SVP_DB_USER\" -d \"$SVP_DB_NAME\" \\\n  -f sovisuplus-YYYY-MM-DD.sql"
  },
  {
    "objectID": "dev/backup-restore-with-dc.html#make-a-sql-dump-1",
    "href": "dev/backup-restore-with-dc.html#make-a-sql-dump-1",
    "title": "Data backup and restoration with Docker Compose",
    "section": "Make a SQL dump",
    "text": "Make a SQL dump\nexport HARVESTER_DB_NAME=${HARVESTER_DB_NAME}\nexport HARVESTER_DB_USER=${HARVESTER_DB_USER}\n\ndocker compose exec -T harvester-db \\\n  pg_dump -U \"$HARVESTER_DB_USER\" -d \"$HARVESTER_DB_NAME\" \\\n    --format=plain \\\n    --no-owner --no-privileges \\\n    --clean --if-exists \\\n  &gt; \"harvester-$(date +%F).sql\""
  },
  {
    "objectID": "dev/backup-restore-with-dc.html#restore-into-docker-harvester-db",
    "href": "dev/backup-restore-with-dc.html#restore-into-docker-harvester-db",
    "title": "Data backup and restoration with Docker Compose",
    "section": "Restore into Docker (harvester-db)",
    "text": "Restore into Docker (harvester-db)\ncat harvester-YYYY-MM-DD.sql | \\\n  docker compose exec -T harvester-db psql -U \"$HARVESTER_DB_USER\" -d \"$HARVESTER_DB_NAME\""
  },
  {
    "objectID": "dev/backup-restore-with-dc.html#restore-into-local-postgres",
    "href": "dev/backup-restore-with-dc.html#restore-into-local-postgres",
    "title": "Data backup and restoration with Docker Compose",
    "section": "Restore into local Postgres",
    "text": "Restore into local Postgres\nCreate target DB/user on your host if needed, then:\nPGPASSWORD=\"$HARVESTER_DB_PASSWORD\" psql \\\n  -h localhost -p 5432 -U \"$HARVESTER_DB_USER\" -d \"$HARVESTER_DB_NAME\" \\\n  -f harvester-YYYY-MM-DD.sql"
  },
  {
    "objectID": "dev/docker-compose.html",
    "href": "dev/docker-compose.html",
    "title": "Docker Compose deployment",
    "section": "",
    "text": "This stack can be deployed in production only if the host is properly secured (firewall enabled, restricted exposed ports, strong credentials, TLS/reverse proxy when needed). Running it on an open host can lead to data leaks or insecure access to internal institutional directories (especially if LDAP is enabled)."
  },
  {
    "objectID": "dev/docker-compose.html#choosing-the-components",
    "href": "dev/docker-compose.html#choosing-the-components",
    "title": "Docker Compose deployment",
    "section": "üîç Choosing the Components",
    "text": "üîç Choosing the Components\nBefore you start, review the components available in map/components.qmd and decide which ones you need.\nThe main Compose file (docker/docker-compose.yaml) is modular. It uses the include directive and profiles to enable only selected components.\n\n\nMain docker-compose.yaml\n\nname: crisalid\n\ninclude:\n  - path: ./neo4j/neo4j.yaml\n    env_file: ./neo4j/.env\n    project_directory: ./neo4j\n  - path: ./apollo/apollo.yaml\n    env_file: ./apollo/.env\n    project_directory: ./apollo\n  - path: ./crisalid-bus/crisalid-bus.yaml\n    env_file: ./crisalid-bus/.env\n    project_directory: ./crisalid-bus\n  - path: ./harvester/harvester.yaml\n    env_file: ./harvester/.env\n    project_directory: ./harvester\n  - path: ./ikg/ikg.yaml\n    env_file: ./ikg/.env\n    project_directory: ./ikg\n  - path: ./cdb/cdb.yaml\n    env_file: ./cdb/.env\n    project_directory: ./cdb\n  - path: ./sovisuplus/sovisuplus.yaml\n    env_file: ./sovisuplus/.env\n    project_directory: ./sovisuplus\n  - path: ./keycloak/keycloak.yaml\n    env_file: ./keycloak/.env\n    project_directory: ./keycloak\n  - path: ./ofelia/ofelia.yaml\n    env_file: ./ofelia/.env\n    project_directory: ./ofelia\n\n\nDEV example\ndocker compose \\\n  -f docker/docker-compose.yaml \\\n  -f docker/docker-compose.dev.yaml \\\n  --profile neo4j \\\n  --profile apollo \\\n  --profile crisalid-bus \\\n  --profile harvester \\\n  --profile ikg \\\n  --profile cdb \\\n  --profile keycloak \\\n  --profile sovisuplus \\\n  --profile ofelia \\\n  up -d\n\n\nPROD example\ndocker compose \\\n  -f docker/docker-compose.yaml \\\n  -f docker/docker-compose.prod.yaml \\\n  --profile neo4j \\\n  --profile apollo \\\n  --profile crisalid-bus \\\n  --profile harvester \\\n  --profile ikg \\\n  --profile cdb \\\n  --profile keycloak \\\n  --profile sovisuplus \\\n  --profile ofelia \\\n  up -d"
  },
  {
    "objectID": "dev/docker-compose.html#preparation-steps",
    "href": "dev/docker-compose.html#preparation-steps",
    "title": "Docker Compose deployment",
    "section": "üß∞ Preparation Steps",
    "text": "üß∞ Preparation Steps\n\n1. üßæ .env Files\nEach directory under docker/ (e.g.¬†apollo, crisalid-bus, ikg, neo4j, cdb, harvester, ‚Ä¶) has its own .env.sample file.\n\nCopy each .env.sample to .env\nFill in appropriate values (hostnames, ports, secrets, etc.)\nThe main docker/.env.sample includes values used by multiple components (like RabbitMQ or Neo4j credentials)\n\n\nIf you plan to connect the CRISalid Directory Bridge (cdb) to your institutional LDAP, make sure to set:\nLDAP_HOST=\nLDAP_BIND_DN=\nLDAP_BIND_PASSWORD=\n\n\n\n2. üîê Configure Basic Authentication for SVP Harvester\nSVP Harvester supports HTTP Basic authentication for both its web interface and REST API.\nAuthentication is enabled by default. You control this behavior via the HARVESTER_ENABLE_BASIC_AUTH environment variable in the harvester‚Äôs .env file.\n\nEnable / disable authentication\nIn docker/harvester/.env:\nENABLE_BASIC_AUTH=true\n\ntrue (default): all /admin/* and /api/* endpoints are protected\nfalse: authentication is disabled (all endpoints are public)\n\n\n‚ö†Ô∏è This authentication mechanism is intended for local development and restricted environments. Always use HTTPS if enabling it in production.\n\n\n\n\nCreate the first user\nUser credentials are stored in a local file mounted into the container:\napp/auth/users.json\nBefore starting the container for the first time, ensure that this file exists and is initialized with an empty JSON object:\nmkdir -p harvester/auth\necho '{}' &gt; harvester/auth/users.json\nOnce the harvester-ui container is running, create an initial user :\ndocker exec -it harvester-ui python scripts/add_basic_user.py admin\nYou will be prompted to enter and confirm a password.\nThe credentials take effect immediately; no container restart is required.\n\n\n\nRemove a user\nTo remove an existing user:\ndocker exec -it harvester-ui python scripts/remove_basic_user.py admin\n\n\n\n\n3. üîß Configure CRISalid Bus\nThis script reads the .env values and generates the RabbitMQ definitions.json file (exchanges, queues, admin user, etc.).\n./docker/configure_crisalid_bus.sh\n\n\n4. üîß Configure CRISalid Directory Bridge (CDB)\nThis script clones the DAGs and runs the Airflow initialization:\n./docker/configure_cdb.sh\n‚ÑπÔ∏è In dev, Airflow GUI admin credentials are set to admin:admin.\nAfter running the script, if you intend to use the CSV mode for structures and people (instead of LDAP), place your data files in:\ndocker/cdb/data/\n‚îú‚îÄ‚îÄ structure.csv\n‚îî‚îÄ‚îÄ people.csv\nSample CSVs:\n\ndocker/cdb/dags/data\n\nFull documentation (in French):\n\nLaboratories\nResearchers\n\n\n\n\n5. üîë Configure Keycloak\nKeycloak is handling authentication within the system. Multiple client applications (such as Sovisu+) can share the same authentication realm. To set up Keycloak in this environment, follow these steps:\n\nGlobal .env Configuration\n\nIn the global .env file, you will find the shared Keycloak configuration variables, such as the realm name ( KEYCLOAK_REALM) and the client secrets (SOVISUPLUS_KEYCLOAK_CLIENT_SECRET). The KEYCLOAK_REALM can be customized ( e.g., crisalid-my-university) for readability.\nExample:\nKEYCLOAK_REALM=crisalid-inst\nSOVISUPLUS_KEYCLOAK_CLIENT_SECRET=MY-SECRET-VALUE\n\nKeycloak Configuration Script\n\nRun the ./configure_keycloak.sh script. This will create the required configuration file from the template ( docker/keycloak/config/crisalid-inst.json.template).\n\nCustomizing Keycloak .env Settings\n\nThe docker/keycloak/.env.sample file provides the environment settings for Keycloak, such as the admin credentials and database configurations. Copy the sample file to .env and modify the settings as needed.\nKEYCLOAK_ADMIN=admin\nKEYCLOAK_ADMIN_PASSWORD=admin\nKEYCLOAK_DB_VENDOR=postgres\nKEYCLOAK_DB_HOST=keycloak-db\nKEYCLOAK_DB_PORT=5432\nKEYCLOAK_DB_NAME=keycloak\nKEYCLOAK_DB_USER=keycloak\nKEYCLOAK_DB_PASSWORD=keycloak\n\nDefine specific URIs in /etc/hosts\n\nTo ensure that Sovisu+ and Keycloak can be accessed correctly, you need to define specific URIs in your /etc/hosts file. This is necessary because Sovisu+ uses OAuth2 with ORCID, which requires a specific hostname even to deliver ‚Äúsandbox‚Äù keys.\n# Add these lines to your /etc/hosts file\n127.0.0.1 sovisuplus.local\n127.0.0.1 keycloak.local\n\n\n6. SoVisu+ custom themes\nSoVisu+ allows you to customize its appearance using themes. You can create your own theme by following these steps: 1. Copy the sample theme directory:\ncp -r sovisuplus/theme-sample sovisuplus/theme\n\nEdit the theme files in sovisuplus/theme to customize text and images according to your institution‚Äôs branding.\n\n\n\n7. SoVisu+ RBAC Roles File\nSoVisu+ comes with a sample RBAC configuration you can customize. Start by copying the sample file and editing it:\ncp sovisuplus/config/rbac.roles.sample.yaml sovisuplus/config/rbac.roles.yaml\n\nEdit sovisuplus/config/rbac.roles.yaml to define your roles by grouping permissions (but dont create new permissions, i.e new actions/subjects/fields as they are used in the code).\nAfter any change, the docker startup script will copy the file to the container and re-seed the roles and permissions in database.\n\n\n\n8. ‚è∞ Configure Ofelia (system-wide scheduler)\nOfelia is the scheduler of the whole CRISalid stack. It runs as its own container and periodically triggers tasks for other containers (by running a CLI command or calling an API endpoint).\nOfelia reads a config file and uses the Docker API to:\n\njob-exec: run a command inside an existing container (via docker exec)\njob-local: run a command directly inside the Ofelia container itself (typically, a curl command to call an API)\n(and job-run: run a one-shot container if needed)\n\nYou can find full documentation here:\n\nDocker Hub: https://hub.docker.com/r/mcuadros/ofelia\nGitHub repository: https://github.com/mcuadros/ofelia/tree/master\n\nIn this deployment, Ofelia is included as its own Docker Compose profile (ofelia) with:\n\na compose file: docker/ofelia/ofelia.yaml\na scheduler config file template: docker/ofelia/config.ini\na configuration template: docker/ofelia/.env.sample\n\n\nCreate Ofelia .env\nIn docker/ofelia/, copy the sample file:\ncp docker/ofelia/.env.sample docker/ofelia/.env\nThe sample content is:\nCONFIG_FILE_NAME=config.ini\nThis variable is used only by Docker Compose to choose which config file to mount inside Ofelia. You can later duplicate config.ini under another name (for example config.dev.ini, config.prod.ini, etc.) and switch by changing CONFIG_FILE_NAME in .env without editing the Compose file:\nCONFIG_FILE_NAME=config.dev.ini\n# or:\n# CONFIG_FILE_NAME=config.prod.ini\n\n\nDefine the scheduled jobs in config.ini\nThe default scheduler configuration file is:\ndocker/ofelia/config.ini\nExample content:\n; file: docker/ofelia/config.ini\n\n[job-exec \"ikg-fetch-pubs\"]\nschedule = @every 2m\ncontainer = crisalid-ikg\ncommand = python -m app.cli people fetch-publication-random\nThis declares a job named ikg-fetch-pubs that:\n\nruns every 2 minutes (@every 2m)\nuses job-exec: it executes the command inside the running crisalid-ikg container\ncalls the internal CLI: python -m app.cli people fetch-publication-random\n\n\n\nRun Ofelia with Docker Compose\nTo start the scheduler along with the rest of the stack, include the ofelia profile, for example:\ndocker compose \\\n...\n  --profile ofelia \\\n  up -d"
  },
  {
    "objectID": "dev/docker-compose.html#communication-with-host-machine",
    "href": "dev/docker-compose.html#communication-with-host-machine",
    "title": "Docker Compose deployment",
    "section": "üîå Communication with Host Machine",
    "text": "üîå Communication with Host Machine\nIf you want to connect external tools (on your host) to the containers, open the necessary ports.\nFor example, to expose RabbitMQ‚Äôs AMQP port on the host machine, edit docker/crisalid-bus/crisalid-bus.yaml and uncomment the 2nd ports line:\nports:\n  - \"${CRISALID_BUS_HTTP_PORT}:15672\"\n#  - \"${CRISALID_BUS_AMQP_PORT}:5672\"\nexpose:\n  - \"${CRISALID_BUS_AMQP_PORT}\""
  },
  {
    "objectID": "dev/docker-compose.html#resetting-containers",
    "href": "dev/docker-compose.html#resetting-containers",
    "title": "Docker Compose deployment",
    "section": "‚ôªÔ∏è Resetting Containers",
    "text": "‚ôªÔ∏è Resetting Containers\nTo stop and delete containers + volumes for one profile, use the same Compose files and profiles that were used during up.\n\nDEV example\ndocker compose \\\n  -f docker/docker-compose.yaml \\\n  -f docker/docker-compose.dev.yaml \\\n  --profile cdb \\\n  down --volumes\n\n\nPROD example\ndocker compose \\\n  -f docker/docker-compose.yaml \\\n  -f docker/docker-compose.prod.yaml \\\n  --profile cdb \\\n  down --volumes\nTo also delete images:\ndocker compose \\\n  -f docker/docker-compose.yaml \\\n  -f docker/docker-compose.prod.yaml \\\n  --profile cdb \\\n  down --volumes --rmi all\n\n\n\nüîé Removing named volumes manually (if needed)\nIf some volumes remain, you can remove them explicitly:\ndocker volume rm postgres-db-volume redis-db-volume data-versioning-redis-volume\ndocker volume rm keycloak_postgres_data\ndocker volume rm svp-db-volume\ndocker volume rm crisalid-bus-volume\ndocker volume rm neo4j-data-volume neo4j-logs-volume neo4j-import-volume neo4j-plugins-volume neo4j-backups-volume"
  },
  {
    "objectID": "dev/docker-compose.html#starting-the-services",
    "href": "dev/docker-compose.html#starting-the-services",
    "title": "Docker Compose deployment",
    "section": "üöÄ Starting the Services",
    "text": "üöÄ Starting the Services\nThe stack is modular. Select the profiles you need and combine the base Compose file with the appropriate environment overlay.\n\nüîß DEV\ndocker compose \\\n  -f docker/docker-compose.yaml \\\n  -f docker/docker-compose.dev.yaml \\\n  --profile neo4j \\\n  --profile apollo \\\n  --profile crisalid-bus \\\n  --profile harvester \\\n  --profile ikg \\\n  --profile cdb \\\n  --profile keycloak \\\n  --profile sovisuplus \\\n  --profile ofelia \\\n  up -d\nIn DEV:\n\nImage tags for components under development are not pinned (typically :latest)\nEnvironment variables such as APP_ENV (when defined) are set to DEV\n\n\n\n\nüè≠ PROD\ndocker compose \\\n  -f docker/docker-compose.yaml \\\n  -f docker/docker-compose.prod.yaml \\\n  --profile neo4j \\\n  --profile apollo \\\n  --profile crisalid-bus \\\n  --profile harvester \\\n  --profile ikg \\\n  --profile cdb \\\n  --profile keycloak \\\n  --profile sovisuplus \\\n  --profile ofelia \\\n  up -d\nIn PROD:\n\nImage tags are pinned to explicit versions\nEnvironment variables such as APP_ENV (when defined) are set to PROD\n\n\nYou can add or remove profiles depending on the components required by the institution."
  },
  {
    "objectID": "dev/docker-compose.html#next-steps",
    "href": "dev/docker-compose.html#next-steps",
    "title": "Docker Compose deployment",
    "section": "‚úÖ Next Steps",
    "text": "‚úÖ Next Steps\nOnce your services are up, follow the component-specific instructions in each section of the documentation. You can now:\n\nAccess the CRISalid Directory Bridge (CDB) UI at http://localhost:8081 (Airflow) and trigger DAGs to import structures and people\nAccess the Neo4j UI at http://localhost:7474 and explore the graph database\nAccess the RabbitMQ UI at http://localhost:15672 with credentials from docker/.env and monitor messages\nAccess SVP Harvester at http://localhost:8000 to monitor publication harvesting\nAccess the Apollo GraphQL UI at http://localhost:4000/graphql to explore the API through Apollo GUI\nAccess Keycloak at http://keycloak.local:8080 to manage users and roles\nAccess SoVisu+ at http://sovisuplus.local:3000 to visualize your data\nStart connecting other CRISalid modules from the host machine\n\nüß≠ Back to Development Index"
  },
  {
    "objectID": "dev/local-tls.html",
    "href": "dev/local-tls.html",
    "title": "Migrate Your Local Dev Instance to TLS/HTTPS",
    "section": "",
    "text": "This section guides you through configuring your local development setup to use HTTPS (TLS) for both SoVisu+ and Keycloak. This is required to enable Saml2 authentication.\n\n\n\n\nRun SoVisu+ on https://sovisuplus.local:3000 and Keycloak on https://keycloak.local:8443, ensuring your local machine and applications trust the self-signed certificates.\n\n\n\n\nWe use mkcert to create locally trusted certificates, avoiding browser warnings. On Ubuntu, libnss3-tools is required for browser trust.\n\nInstall Prerequisites and mkcert:\n\nsudo apt update\nsudo apt install libnss3-tools\ncurl -JLO \"[https://dl.filippo.io/mkcert/latest?for=linux/amd64](https://dl.filippo.io/mkcert/latest?for=linux/amd64)\"\nchmod +x mkcert-v*-linux-amd64\nsudo mv mkcert-v*-linux-amd64 /usr/local/bin/mkcert\n\nInitialize Local Certificate Authority:\n\nmkcert -install\n\nGenerate Certificates: Create a directory to store the files and generate a certificate that covers both domains:\n\nThe name and location of ‚Äúlocal-certs‚Äù matters because it will be referenced later.\nmkdir -p ~/local-certs\ncd ~/local-certs\nmkcert sovisuplus.local keycloak.local localhost 127.0.0.1 ::1\n\nFix Key Permissions for Docker:\n\nThe Keycloak container needs read access to the private key.\nsudo chmod a+r ~/local-certs/sovisuplus.local+4-key.pem\n\n\n\n\nYou need to update several environment variables in your Next.js application‚Äôs .env file and the Keycloak Docker configuration.\n\n\nUpdate the URLs to reflect the new https scheme and the Keycloak HTTPS port (8443).\n# Before (HTTP)\n# WS_SCHEME=ws\n# KEYCLOAK_ISSUER=\"[http://keycloak.local:8080/realms/crisalid-inst](http://keycloak.local:8080/realms/crisalid-inst)\"\n# KEYCLOAK_PUBLIC_URL=\"[http://keycloak.local:8080/realms/crisalid-inst](http://keycloak.local:8080/realms/crisalid-inst)\"\n# NEXTAUTH_URL=\"[http://sovisuplus.local:3000/api/auth](http://sovisuplus.local:3000/api/auth)\"\n\n# After (HTTPS)\nWS_SCHEME=wss\nKEYCLOAK_ISSUER=\"[https://keycloak.local:8443/realms/crisalid-inst](https://keycloak.local:8443/realms/crisalid-inst)\"\nKEYCLOAK_PUBLIC_URL=\"[https://keycloak.local:8443/realms/crisalid-inst](https://keycloak.local:8443/realms/crisalid-inst)\"\nNEXTAUTH_URL=\"[https://sovisuplus.local:3000/api/auth](https://sovisuplus.local:3000/api/auth)\"\n\n# UNCOMMENT THIS LINE to allow the Next.js server (Node.js) to communicate\n# with Keycloak without complaining about the self-signed certificate.\nNODE_TLS_REJECT_UNAUTHORIZED=0\n\n\n\nUpdate the scheme and port for both Keycloak and SoVisu+ to use HTTPS.\n# Before\n# KEYCLOAK_SCHEME=http\n# KEYCLOAK_PORT=8080\n# SOVISUPLUS_SCHEME=http\n\n# After\nKEYCLOAK_SCHEME=https\nKEYCLOAK_PORT=8443\nSOVISUPLUS_SCHEME=https\n\n\n\nEnsure the hostname enforcement variables are set correctly for the TLS setup.\n# Invert these values from the default/sample:\nKEYCLOAK_HTTP_ENABLED=false\nKEYCLOAK_HOSTNAME_STRICT_HTTPS=true\n\n\n\n\n\nTo run the Next.js development server over HTTPS, we need to use the --experimental-https flag along with the certificate files you generated. This is done via a dedicated script in package.json.\n\npackage.json contains a ‚Äòdev-tls‚Äô script:\n\n\"scripts\": {\n  \"dev\": \"next dev -H sovisuplus.local -p 3000\",\n  \"dev-tls\": \"next dev --experimental-https --experimental-https-key ~/local-certs/sovisuplus.local+4-key.pem --experimental-https-cert ~/local-certs/sovisuplus.local+4.pem -H sovisuplus.local -p 3000\"\n}\n\nHow ro Run the Dev Server with TLS:\n\nInstead of npm run dev, use:\nnpm run dev-tls\n\nReference: This approach uses built-in Next.js functionality. See Vercel KB: Access Next.js localhost HTTPS certificate self-signed.\n\n\n\n\n\nYou must now modify your docker compose environment to start Keycloak with its HTTPS configuration, which is handled via a dedicated keycloak-tls profile.\n\nDestroy the Old DB:\n\nKeycloak will write the old HTTP configuration into its database. It‚Äôs safest to destroy the database volume to force a clean, HTTPS-based configuration import.\n# Ensure Docker compose is stopped\n# Remove the persisted DB volume\nsudo rm -rf docker/keycloak/postgres-data\n\nRenew Configuration Script:\n\nRe-run your configuration script to ensure the Keycloak realm file (crisalid-inst.json) contains the new https:// URLs.\nEnsure all the environment variables are set correctly before running the script (step 2).\n./configure_keycloak.sh\n\nLaunch Docker Compose with the TLS Profile:\n\nReplace --profile keycloak with the dedicated --profile keycloak-tls when starting Docker Compose.\ndocker compose \\\n  # ... all other profiles ...\n  --profile keycloak-tls \\ \n  --profile sovisuplus-db \\\n  up --remove-orphans\n\n\n\nKeycloak: Should be accessible securely at https://keycloak.local:844.\nSoVisu+: Should be accessible securely at https://sovisuplus.local:3000."
  },
  {
    "objectID": "dev/local-tls.html#goal",
    "href": "dev/local-tls.html#goal",
    "title": "Migrate Your Local Dev Instance to TLS/HTTPS",
    "section": "",
    "text": "Run SoVisu+ on https://sovisuplus.local:3000 and Keycloak on https://keycloak.local:8443, ensuring your local machine and applications trust the self-signed certificates."
  },
  {
    "objectID": "dev/local-tls.html#step-1-install-certificate-authority-mkcert",
    "href": "dev/local-tls.html#step-1-install-certificate-authority-mkcert",
    "title": "Migrate Your Local Dev Instance to TLS/HTTPS",
    "section": "",
    "text": "We use mkcert to create locally trusted certificates, avoiding browser warnings. On Ubuntu, libnss3-tools is required for browser trust.\n\nInstall Prerequisites and mkcert:\n\nsudo apt update\nsudo apt install libnss3-tools\ncurl -JLO \"[https://dl.filippo.io/mkcert/latest?for=linux/amd64](https://dl.filippo.io/mkcert/latest?for=linux/amd64)\"\nchmod +x mkcert-v*-linux-amd64\nsudo mv mkcert-v*-linux-amd64 /usr/local/bin/mkcert\n\nInitialize Local Certificate Authority:\n\nmkcert -install\n\nGenerate Certificates: Create a directory to store the files and generate a certificate that covers both domains:\n\nThe name and location of ‚Äúlocal-certs‚Äù matters because it will be referenced later.\nmkdir -p ~/local-certs\ncd ~/local-certs\nmkcert sovisuplus.local keycloak.local localhost 127.0.0.1 ::1\n\nFix Key Permissions for Docker:\n\nThe Keycloak container needs read access to the private key.\nsudo chmod a+r ~/local-certs/sovisuplus.local+4-key.pem"
  },
  {
    "objectID": "dev/local-tls.html#step-2-configure-environment-variables",
    "href": "dev/local-tls.html#step-2-configure-environment-variables",
    "title": "Migrate Your Local Dev Instance to TLS/HTTPS",
    "section": "",
    "text": "You need to update several environment variables in your Next.js application‚Äôs .env file and the Keycloak Docker configuration.\n\n\nUpdate the URLs to reflect the new https scheme and the Keycloak HTTPS port (8443).\n# Before (HTTP)\n# WS_SCHEME=ws\n# KEYCLOAK_ISSUER=\"[http://keycloak.local:8080/realms/crisalid-inst](http://keycloak.local:8080/realms/crisalid-inst)\"\n# KEYCLOAK_PUBLIC_URL=\"[http://keycloak.local:8080/realms/crisalid-inst](http://keycloak.local:8080/realms/crisalid-inst)\"\n# NEXTAUTH_URL=\"[http://sovisuplus.local:3000/api/auth](http://sovisuplus.local:3000/api/auth)\"\n\n# After (HTTPS)\nWS_SCHEME=wss\nKEYCLOAK_ISSUER=\"[https://keycloak.local:8443/realms/crisalid-inst](https://keycloak.local:8443/realms/crisalid-inst)\"\nKEYCLOAK_PUBLIC_URL=\"[https://keycloak.local:8443/realms/crisalid-inst](https://keycloak.local:8443/realms/crisalid-inst)\"\nNEXTAUTH_URL=\"[https://sovisuplus.local:3000/api/auth](https://sovisuplus.local:3000/api/auth)\"\n\n# UNCOMMENT THIS LINE to allow the Next.js server (Node.js) to communicate\n# with Keycloak without complaining about the self-signed certificate.\nNODE_TLS_REJECT_UNAUTHORIZED=0\n\n\n\nUpdate the scheme and port for both Keycloak and SoVisu+ to use HTTPS.\n# Before\n# KEYCLOAK_SCHEME=http\n# KEYCLOAK_PORT=8080\n# SOVISUPLUS_SCHEME=http\n\n# After\nKEYCLOAK_SCHEME=https\nKEYCLOAK_PORT=8443\nSOVISUPLUS_SCHEME=https\n\n\n\nEnsure the hostname enforcement variables are set correctly for the TLS setup.\n# Invert these values from the default/sample:\nKEYCLOAK_HTTP_ENABLED=false\nKEYCLOAK_HOSTNAME_STRICT_HTTPS=true"
  },
  {
    "objectID": "dev/local-tls.html#step-3-configure-next.js-for-tls",
    "href": "dev/local-tls.html#step-3-configure-next.js-for-tls",
    "title": "Migrate Your Local Dev Instance to TLS/HTTPS",
    "section": "",
    "text": "To run the Next.js development server over HTTPS, you need to use the --experimental-https flag along with the certificate files you generated. This is best done via a new script in package.json.\n\npackage.json contains the new script:\n\n\"scripts\": {\n  \"dev\": \"next dev -H sovisuplus.local -p 3000\",\n  \"dev-tls\": \"next dev --experimental-https --experimental-https-key ~/local-certs/sovisuplus.local+4-key.pem --experimental-https-cert ~/local-certs/sovisuplus.local+4.pem -H sovisuplus.local -p 3000\"\n}\n\nHow ro Run the Dev Server with TLS:\n\nInstead of npm run dev, use:\nnpm run dev-tls\n\nReference: This approach uses built-in Next.js functionality. See Vercel KB: Access Next.js localhost HTTPS certificate self-signed."
  },
  {
    "objectID": "dev/local-tls.html#step-4-configure-and-restart-keycloak-tls-profile",
    "href": "dev/local-tls.html#step-4-configure-and-restart-keycloak-tls-profile",
    "title": "Migrate Your Local Dev Instance to TLS/HTTPS",
    "section": "",
    "text": "You must now modify your docker compose environment to start Keycloak with its HTTPS configuration, which is handled via a dedicated keycloak-tls profile.\n\nDestroy the Old DB:\n\nKeycloak will write the old HTTP configuration into its database. It‚Äôs safest to destroy the database volume to force a clean, HTTPS-based configuration import.\n# Ensure Docker compose is stopped\n# Remove the persisted DB volume\nsudo rm -rf docker/keycloak/postgres-data\n\nRenew Configuration Script:\n\nRe-run your configuration script to ensure the Keycloak realm file (crisalid-inst.json) contains the new https:// URLs.\nEnsure all the environment variables are set correctly before running the script (step 2).\n./configure_keycloak.sh\n\nLaunch Docker Compose with the TLS Profile:\n\nReplace --profile keycloak with the dedicated --profile keycloak-tls when starting Docker Compose.\ndocker compose \\\n  # ... all other profiles ...\n  --profile keycloak-tls \\ \n  --profile sovisuplus-db \\\n  up --remove-orphans\n\n\n\nKeycloak: Should be accessible securely at https://keycloak.local:844.\nSoVisu+: Should be accessible securely at https://sovisuplus.local:3000."
  },
  {
    "objectID": "dev/local-tls.html#step-3-run-next.js-dev-server-with-https",
    "href": "dev/local-tls.html#step-3-run-next.js-dev-server-with-https",
    "title": "Migrate Your Local Dev Instance to TLS/HTTPS",
    "section": "",
    "text": "To run the Next.js development server over HTTPS, we need to use the --experimental-https flag along with the certificate files you generated. This is done via a dedicated script in package.json.\n\npackage.json contains a ‚Äòdev-tls‚Äô script:\n\n\"scripts\": {\n  \"dev\": \"next dev -H sovisuplus.local -p 3000\",\n  \"dev-tls\": \"next dev --experimental-https --experimental-https-key ~/local-certs/sovisuplus.local+4-key.pem --experimental-https-cert ~/local-certs/sovisuplus.local+4.pem -H sovisuplus.local -p 3000\"\n}\n\nHow ro Run the Dev Server with TLS:\n\nInstead of npm run dev, use:\nnpm run dev-tls\n\nReference: This approach uses built-in Next.js functionality. See Vercel KB: Access Next.js localhost HTTPS certificate self-signed."
  },
  {
    "objectID": "dev/export-kc-realm.html",
    "href": "dev/export-kc-realm.html",
    "title": "Export Keycloak Realm",
    "section": "",
    "text": "To update the Keycloak configuration template in this repository, you need to export the current realm from your running Keycloak instance. This guide walks you through exporting the crisalid-inst realm.\n\n\n\n\n\nThe Keycloak service must be running and accessible via Docker Compose.\nThe realm you wish to export must be named crisalid-inst.\n\n\n\n\n\nFollow these steps to extract the realm configuration (clients, roles, and settings) from the running container and save it to your local configuration folder (docker/keycloak/config).\n\n\nConnect to your running Keycloak service container via the terminal:\ndocker exec -it keycloak /bin/bash\n\n\n\nInside the container, use kc.sh export to create the realm file. We specify --users skip to ensure the export only includes configuration metadata, not specific user data.\n/opt/keycloak/bin/kc.sh export \\\n  --dir /opt/keycloak/data/export \\\n  --realm crisalid-inst \\\n  --users skip\nA file named crisalid-inst-realm.json is created in the container‚Äôs export folder.\n\n\n\nExit the container, and run the following command on your host machine to copy the file into your local configuration directory:\nFrom docker/keycloak directory, run:\nmkdir -p realm-export # or whatever folder you want to use\ndocker cp keycloak:/opt/keycloak/data/export/crisalid-inst-realm.json ./realm-export/\n\nResult: You now have the exported realm file at docker/keycloak/realm-export/crisalid-inst-realm.json. You can use this file to update your configuration template located at docker/keycloak/config/crisalid-inst.json.template."
  },
  {
    "objectID": "dev/export-kc-realm.html#prerequisites",
    "href": "dev/export-kc-realm.html#prerequisites",
    "title": "Export Keycloak Realm",
    "section": "",
    "text": "The Keycloak service must be running and accessible via Docker Compose.\nThe realm you wish to export must be named crisalid-inst."
  },
  {
    "objectID": "dev/export-kc-realm.html#export-procedure",
    "href": "dev/export-kc-realm.html#export-procedure",
    "title": "Export Keycloak Realm",
    "section": "",
    "text": "Follow these steps to extract the realm configuration (clients, roles, and settings) from the running container and save it to your local configuration folder (docker/keycloak/config).\n\n\nConnect to your running Keycloak service container via the terminal:\ndocker exec -it keycloak /bin/bash\n\n\n\nInside the container, use kc.sh export to create the realm file. We specify --users skip to ensure the export only includes configuration metadata, not specific user data.\n/opt/keycloak/bin/kc.sh export \\\n  --dir /opt/keycloak/data/export \\\n  --realm crisalid-inst \\\n  --users skip\nA file named crisalid-inst-realm.json is created in the container‚Äôs export folder.\n\n\n\nExit the container, and run the following command on your host machine to copy the file into your local configuration directory:\nFrom docker/keycloak directory, run:\nmkdir -p realm-export # or whatever folder you want to use\ndocker cp keycloak:/opt/keycloak/data/export/crisalid-inst-realm.json ./realm-export/\n\nResult: You now have the exported realm file at docker/keycloak/realm-export/crisalid-inst-realm.json. You can use this file to update your configuration template located at docker/keycloak/config/crisalid-inst.json.template."
  },
  {
    "objectID": "map/harvesting.html",
    "href": "map/harvesting.html",
    "title": "Harvesting Configuration",
    "section": "",
    "text": "This page describes how harvesting is triggered and configured in CRISalid, and how responsibilities are split between the IKG and the Harvester microservices."
  },
  {
    "objectID": "map/harvesting.html#from-request-to-execution",
    "href": "map/harvesting.html#from-request-to-execution",
    "title": "Harvesting Configuration",
    "section": "üîÅ From Request to Execution",
    "text": "üîÅ From Request to Execution\n\nThe IKG publishes a harvesting request on the publications exchange\nThe message contains:\n\nthe target entity (for now, only person harvesting is supported)\n\nthe identifiers of the person to harvest (e.g.¬†ORCID, IdRef, etc.)\n\nthe list of requested harvesters (derived from HARVESTERS)\n\n\n{\n  \"name\": \"John Doe\",\n  \"identifiers\": [\n    {\n      \"type\": \"orcid\",\n      \"value\": \"0000-0002-1825-0097\"\n    },\n    {\n      \"type\": \"idref\",\n      \"value\": \"123456789\"\n    }\n  ]\n}\n\nThe Harvester service:\n\nreceives the message\nfilters the requested harvesters against those defined in harvesters.yaml\nexecutes each matching harvester in parallel"
  },
  {
    "objectID": "map/harvesting.html#overview",
    "href": "map/harvesting.html#overview",
    "title": "Harvesting Configuration",
    "section": "üåê Overview",
    "text": "üåê Overview\nHarvesting is triggered by messages published on the RabbitMQ publications exchange by the IKG (Institutional Knowledge Graph) service. The Harvester UI provides observability only (although it can be used to trigger harvesting requests manually, but only for testing purposes).\nThe IKG publishes harvesting requests to the RabbitMQ publications exchange.\nEach message represents one harvesting job.\nRequests can be triggered:\n\nManually, via IKG CLI tools\nAutomatically, via Ofelia (Docker Compose) or Kubernetes Jobs / CronJobs (which in turn call IKG CLI tools)"
  },
  {
    "objectID": "map/harvesting.html#configuration",
    "href": "map/harvesting.html#configuration",
    "title": "Harvesting Configuration",
    "section": "‚öôÔ∏è Configuration",
    "text": "‚öôÔ∏è Configuration\nHarvesting behavior is configured at two distinct levels:\n\nIn the IKG, which decides when harvesting is triggered and which harvesters are requested\nIn the Harvester service, which defines which harvesters exist and if they are executed\n\n\n\nA. IKG Configuration\nAt the IKG level, the list of harvesters to include in each harvesting request is configurable.\n\nHARVESTERS Environment Variable\nThe environment variable HARVESTERS defines the list of harvesters that will be added to harvesting requests published on the publications exchange.\n\nIt can be defined in:\n\nthe IKG runtime environment\nDocker Compose manifests (e.g.¬†docker/ikg/ikg.yaml)\nKubernetes manifests (ConfigMaps)\n\nIt overrides the default configuration when set\n\n\n\nDefault Configuration\nA default list of harvesters is defined in the IKG settings:\n[\n  \"idref\",\n  \"scanr\",\n  \"hal\",\n  \"openalex\",\n  \"scopus\"\n]\nThis default is used when HARVESTERS is not explicitly set.\n\nüîó See the default configuration in the IKG repository: (link to GitHub settings file)\n\n\n\n\n\nB. Harvester Service Configuration\nAt the Harvester level, configuration focuses on which harvesters exist and the link with their implementation.\n\nharvesters.yaml\nThe list of available harvesters is defined in the configuration file:\nharvesters.yaml\nThis file declares:\n\nThe identifier of each harvester (e.g.¬†hal, scanr, idref)\nThe associated implementation\n\nOnly harvesters declared in harvesters.yaml can be executed.\n  # --HARVESTER PLATFORM IDENTIFIERS--\n  - name: idref\n    module: app.harvesters.idref.idref_harvester_factory\n    class: IdrefHarvesterFactory\n  - name: scanr\n    module: app.harvesters.scanr.scanr_harvester_factory\n    class: ScanrHarvesterFactory\n  - name: hal\n    module: app.harvesters.hal.hal_harvester_factory\n    class: HalHarvesterFactory\n  - name: openalex\n    module: app.harvesters.open_alex.open_alex_harvester_factory\n    class: OpenAlexHarvesterFactory\n  - name: scopus\n    module: app.harvesters.scopus.scopus_harvester_factory\n    class: ScopusHarvesterFactory"
  },
  {
    "objectID": "map/harvesting.html#configuration-execution",
    "href": "map/harvesting.html#configuration-execution",
    "title": "Harvesting Configuration",
    "section": "‚öôÔ∏è Configuration & Execution",
    "text": "‚öôÔ∏è Configuration & Execution\nHarvesting is configured at two levels:\n\nIKG: decides when harvesting runs and which harvesters are requested\nHarvester: defines which harvesters exist and whether they can run\n\n\nIKG ‚Äî Requested Harvesters\nThe HARVESTERS environment variable defines which harvesters are included in harvesting requests published on the publications exchange.\n\nCan be set via:\n\nruntime environment\nDocker Compose (e.g.¬†docker/ikg/ikg.yaml)\nKubernetes ConfigMaps\n\nOverrides the default when defined\n\nDefault configuration:\n[\n  \"idref\",\n  \"scanr\",\n  \"hal\",\n  \"openalex\",\n  \"scopus\"\n]\n\nüîó Defined in the IKG settings (link to GitHub settings file)\n\n\n\n\nHarvester ‚Äî Available Harvesters\nThe Harvester service declares available harvesters in:\nharvesters.yaml\nThis file binds harvester identifiers to their implementations.\n- name: idref\n  module: app.harvesters.idref.idref_harvester_factory\n  class: IdrefHarvesterFactory\n\n- name: scanr\n  module: app.harvesters.scanr.scanr_harvester_factory\n  class: ScanrHarvesterFactory\n\n- name: hal\n  module: app.harvesters.hal.hal_harvester_factory\n  class: HalHarvesterFactory\n\n- name: openalex\n  module: app.harvesters.open_alex.open_alex_harvester_factory\n  class: OpenAlexHarvesterFactory\n\n- name: scopus\n  module: app.harvesters.scopus.scopus_harvester_factory\n  class: ScopusHarvesterFactory\nOnly harvesters declared in harvesters.yaml can be executed."
  },
  {
    "objectID": "map/harvesting.html#execution-rules",
    "href": "map/harvesting.html#execution-rules",
    "title": "Harvesting Configuration",
    "section": "üîÅ Execution Rules",
    "text": "üîÅ Execution Rules\n\nThe IKG publishes a harvesting request on the publications exchange\nThe message contains:\n\nthe type of entity (currently: person only)\na flag indicating whether a reply is expected\na flag for ‚Äúidentifiers safe mode‚Äù (if true, the alignment of identifiers is not recorded in the harvesters database)\na list of event types to be reported (e.g.¬†created, updated, deleted, unchanged)\na list of requested harvesters (e.g.¬†idref, scanr, hal, openalex, scopus)\nthe fields of the entity to be harvested, including a list of identifiers with their types and values :\n\nname : displayed in the Harvester UI but not used for harvesting\nidentifiers : used to determine which harvesters can run (e.g.¬†openalex requires an ORCID identifier) and to perform the actual harvesting\n\n\n\n{\n  \"type\": \"person\",\n  \"reply\": true,\n  \"identifiers_safe_mode\": false,\n  \"events\": [\n    \"created\",\n    \"updated\",\n    \"deleted\",\n    \"unchanged\"\n  ],\n  \"harvesters\": [\n    \"idref\",\n    \"scanr\",\n    \"hal\",\n    \"openalex\",\n    \"scopus\"\n  ],\n  \"fields\": {\n    \"name\": \"John Doe\",\n    \"identifiers\": [\n      {\n        \"type\": \"orcid\",\n        \"value\": \"0000-0002-1825-0097\"\n      },\n      {\n        \"type\": \"idref\",\n        \"value\": \"123456789\"\n      }\n    ]\n  }\n}\n\nFor each requested harvester, the Harvester service checks:\n\nif the harvester is declared in harvesters.yaml\nif a compatible identifier is present\n\n\nA harvester is executed only if a suitable identifier is available (e.g.¬†openalex requires an ORCID and will not run with a HAL identifier).\n\nAll eligible harvesters are executed in parallel"
  },
  {
    "objectID": "dev/docker-compose.html#security-note",
    "href": "dev/docker-compose.html#security-note",
    "title": "Docker Compose deployment",
    "section": "",
    "text": "This stack can be deployed in production only if the host is properly secured (firewall enabled, restricted exposed ports, strong credentials, TLS/reverse proxy when needed). Running it on an open host can lead to data leaks or insecure access to internal institutional directories (especially if LDAP is enabled)."
  },
  {
    "objectID": "dev/docker-compose.html#environment-specific-compose-files-dev-prod",
    "href": "dev/docker-compose.html#environment-specific-compose-files-dev-prod",
    "title": "Docker Compose deployment",
    "section": "üåç Environment-specific Compose files (DEV / PROD)",
    "text": "üåç Environment-specific Compose files (DEV / PROD)\nThe repository ships with additional Compose files to adapt the same modular stack to different environments:\n\nDEV overlay (docker-compose.dev.yaml)\n\nDocker image tags for components under active development are not pinned (typically :latest).\nEnvironment variables that depend on the environment (e.g.¬†APP_ENV, when present) are set to DEV.\n\nPROD overlay (docker-compose.prod.yaml)\n\nDocker image tags are pinned (explicit versions) to ensure reproducible deployments.\nEnvironment variables that depend on the environment (e.g.¬†APP_ENV, when present) are set to PROD.\n\n\nUse the overlays by combining files with -f (the overlay only overrides what differs from the base file)."
  }
]